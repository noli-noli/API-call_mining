import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
import os


pd.set_option("display.max_columns",None)
pd.set_option("display.max_rows",None)


DATASET_PATH = os.path.join("/workspace","datasets","brazilian-malware-dataset-master","goodware-malware","goodware.csv")
READ_DATASET = pd.read_csv(DATASET_PATH)


#print(READ_DATASET.head())
#print(READ_DATASET[["Name","ImportedSymbols"]].head())
#print(READ_DATASET["ImportedSymbols"].head().unique())


TOKENIZER = Tokenizer()

TOKENIZER.fit_on_texts(READ_DATASET["ImportedSymbols"])
TOKENIZER_SEQUENCES = TOKENIZER.texts_to_sequences(READ_DATASET["ImportedSymbols"].head())
#print(TOKENIZER_SEQUENCES)
#print(TOKENIZER.word_index)
print(TOKENIZER.word_counts)
print("vocabulary size:",len(TOKENIZER.word_index))